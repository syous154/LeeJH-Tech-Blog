---
{"dg-publish":true,"permalink":"/cs/2/","created":"2025-03-01T16:55:32.930+09:00","updated":"2025-03-01T17:14:05.599+09:00"}
---

## Metric의 종류

- 분류 작업을 위한 Metric
	- 정확도(Accuracy)
		모델의 예측이 얼마나 정확한지를 의미
		예측이 맞은 경우 / 전체 예측의 수 를 통해서 계산한다
		만약에 정답이 한쪽에 몰려 있을 경우 정확도는 의미가 없어질 수 있다.
	
	- 오차행렬
		모델이 예측을 얼마나 헷갈려하는지를 알려주는 지표
		주로 이진 분류에서 많이 사용한다
		![Pasted image 20250301133345.png](/img/user/images/Pasted%20image%2020250301133345.png)
		- 정밀도(Precision), 재현율(Recall)
			Positive 데이터 예측성능에 초점을 맞춘 평가지표이다.
			- 정밀도: 예측을 Positive일 때 실제로 Positive인 경우
				TP / (FP + TP)
			- 재현율: 실제로 Positive일 때 예측도 Positive인 경우
				TP / (FN + TP)
			
			정밀도와 재현율은 trad-off 관계를 갖는다. 따라서 가장 좋은 경우는 정밀도, 재현율 모두 높은 성능을 기록할 떄이다.
		
		- F1 score
			정밀도와 재현율 둘 중에 어느 한쪽으로 치우치지 않고 둘 다 균형을 이루는 것을 나타낸 지표
			F1 score를 통해 정밀도와 재현율의 조화평균으로 계산할 수 있다.
			![Pasted image 20250301134012.png](/img/user/images/Pasted%20image%2020250301134012.png)
		
		- ROC-AUC
			ROC는 **FPR(False Positive Rate)가 변할 때 TPR(True Positive Rate)가 어떻게 변하는지를 나타내는 곡선**을 말한다.
			**FPR**이란 **FP / (FP + TN)이고, TPR은 TP / (FN + TP)으로 재현율을 말한다.
			
			그럼 어떻게 FPR을 움직일까? 
			
			바로 분류 결정 임계값을 변경함으로써 움직일 수 있다. FPR이 0이 되려면 임계값을 1로 설정하면 된다. 그럼 긍정의 기준이 높으니 모두 부정으로 예측될 것이다. 반대로 1이 되려면 임계값을 0으로 설정하여 모두 긍정으로 예측시키면 된다. 이렇게 임계값을 움직이면서 나오는 FPR과 TPR을 각각 x와 y 좌표로 두고 그린 곡선이 ROC이다.
			
			AUC는 ROC 곡선의 넓이를 의미한다. AUC가 넓을 수록 Perfect Classifier에 가까워지기 때문에 성능이 좋다고 말할 수 있다.
			
			![Pasted image 20250301134036.png](/img/user/images/Pasted%20image%2020250301134036.png)

- 회귀 작업을 위한 Metirc
	- MAE: Mean Absolute Error를 의미하며 **예측값과 정답값 사이의 차이릐 절대값 평균**을 말한다.
		$MAE = \frac{1}{N} \sum_{i=1}^{N} | y_i - \hat{y}_i |$
	
	- MSE: Meas Square Error를 의미하며 **예측값과 정답값 사이의 차이를 제곱의 평균**을 말하여, MAE와 다르게 제곱을 해서 이상치에 더 민감하다.
		$MSE = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2$
	
	- RMSE: Root Mean Square Error는 **MSE에 루트를 씌운 값**을 말한다.
		$RMSE = \sqrt{MSE} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2}$
	
	- R Squared: **분산을 기반으로 예측 성능을 평가하는 지표**를 말한다. 정답값의 분산 대비 예측값의 분산 비율을 지표로 하며, 1에 가까울수록 정확도가 높다.
		$R^2 = 1 - \frac{\sum_{i=1}^{N} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{N} (y_i - \bar{y})^2}$

---
## 정규화의 필요성

- 정규화: 개별 feature의 크기를 모두 똑같은 단위로 변경하는 것을 말한다. 
- 사용 이유: feature 별로 scale차이가 심하게 나는 경우 값이 큰 feature가 더 중요하게 여겨질 수 있기 때문이다. 이러한 문제를 방지하기 위해서 모두 동일한 scale을 반영하도록 해야한다.

- 방법
	- Min-Max 정규화
		각 feature의 최솟값을 0, 최댓값을 1로 두고 변환하는 방법이다.
		값을 $x$, 최솟값을 $min$, 최댓값을 $max$로 가정하면 정규화 하는 공식은 $\frac{x - min}{max - min}$와 같다.
	
	- Z-score 정규화
		각 feature의 표준편차와 평균으로 값을 정규화하는 방법이다.
		정규화하는 공식은 $\frac{x - mean}{std}$로 나타낼 수 있다.

---
## Local minimum과 Global minimum의 차이

![Pasted image 20250301140150.png](/img/user/images/Pasted%20image%2020250301140150.png)
- Global minimum: 비용함수에서 에러가 최소화 되는 지점. 즉, 우리가 찾고자하는 지점을 말한다.
- Local minimum: 에러가 최소가 될 수 있는 지점 중에서 Global minimum을 제외한 지점을 말한다. 
	이는 우리가 Global minimum에 도달했다고 착각할 수 있게 한다. 따라서 Momemtum이나 lr등을 잘 적용하여 벗어나도록 해야한다.

---
## 차원의 저주

![Pasted image 20250301140703.png](/img/user/images/Pasted%20image%2020250301140703.png)
- 차원의 저주: 데이터 차원이 증가할 수록 데이터가 있는 공간의 크기가 기하급수적으로 커지며 데이터 간의 거리 또한 멀어지면서 희소한 구조를 갖게되는 현상을 말한다. &rarr; 데이터의 특징을 모두 잃어버리게 됨
	해결하기 위해서는 데이터를 더 추가하거나 PCA와 같은 차원 축소 알고리즘을 사용해야한다.

---
## 차원 축소 기법

- Feature Selection(특징 선택)
	특정 feature에서 종속성이 강한 불필요한 feature는 제거하고 데이터의 특징을 잘 표현하는 주요 feature만 선택하는 방법이다.

- Feature Extraction(특징 추출)
	기존의 feature를 저차원의 feature로 압축하여 함축적으로 설명할 수 있도록 저차원으로 매핑 시키는 것을 의미한다. 대표적으로 PCA등이 있다.

---
## PCA는 차원축소? 데이터 압축? 노이즈 제거?

- PCA(Principle Component Analysis)
	입력 데이터의 공분산 행렬을 기반으로 고유벡터를 생성하고 이렇게 구한 고유 벡터에 입력 데이터를 선형 변환하여 차원을 축소하는 방법이다. 차원은 곧 feature를 의미하기에 데이터 압축기법이라 볼 수 있다.
	
	또한 고유값이 가장 큰 순서로 주성분 벡터를 추출하는데 이를 통해 가장 먼저 뽑힌 벡터가 데이터를 제일 잘 설명할 수 있기 떄문에 노이즈 제거 기법이라고도 불린다.

---
## Markov Chain

![Pasted image 20250301141603.png](/img/user/images/Pasted%20image%2020250301141603.png)
- Markov Chain: 마코프 성질을 지닌 이산 확률 과정을 의미한다 &rarr; **확률변수(random variable)가 어떤 상태(state)에 도달할 확률이 오직 바로 이전 시점의 상태(state)에 달려 있는 경우**를 가리킨다.
  
- Markov Property: n+1회의 상태는 오직 n회에서의 상태, 혹은 그 이전 일정 기간의 상태에만 영향을 받는 것을 의미한다.

- 예를 들어, 오늘의 날씨가 어제의 날씨에만 의존하면 1차 마코프 체인, 이틀 전까지의 날씨에만 의존하면 2차 마코프 체인이다.

- Markov Model: 위의 가정하에 확률적 모델을 만든 것으로 가장 먼저 각 상태를 정의, 그 다음 상태 전이 확률을 정의한다.
	  ![Pasted image 20250301143547.png](/img/user/images/Pasted%20image%2020250301143547.png)

---
## SVM

- SVM: Surpport Vector Mechine은 데이터가 사상된 공간에서 경계로 표현되며 **공간상에서 존재하는 여러 경계 중 가장 큰 폭을 가지는 경계를 의미한다.**
	![Pasted image 20250301143748.png](/img/user/images/Pasted%20image%2020250301143748.png)

- SVD 장단점

| 장점                           | 단점                                       |
| ---------------------------- | ---------------------------------------- |
| 분류와 회귀에 모두 사용할 수 있다.         | 데이터 전처리와 매개변수 설정에 따라 정확도가 달라질 수 있다.      |
| 신경망 기법에 비해 과적합 정도가 낮다.       | 예측이 어떻게 이루어지는지에 대한 이해와 모델에 대한 해석이 어렵다.   |
| 예측의 정확도가 높다.                 | 대용량 데이터에 대한 모델 구축 시 속도가 느리며,메모리 할당량이 크다. |
| 저차원과 고차원 데이터에 대해서 모두 잘 작동한다. |                                          |

- 마진(Margin): plus-plane과 minus-plane 사이의 거리를 의미하며 최적의 결정 경계는 마진을 최대화한다.

- 비선형 분류에서 SVM
	커널 트릭(kernel trick)이라는 방법을 이용해 선형 분류가 가능한 고차원의 공간으로 매핑하여 두 범주를 구분하는 초평면을 찾는 방법이다. (Kernel-SVM)
	![Pasted image 20250301144245.png](/img/user/images/Pasted%20image%2020250301144245.png)
	- Kernel Trick (커널 트릭): 커널 함수를 이용해 저차원 공간을 고차원 공간으로 매핑해주는 작업

---
## Naive Bayes(나이브 베이즈)

- 데이터에서 변수들에 대한 **조건부 독립을 가정**하는 알고리즘으로 클래스에 대한 사전 정보와 데이터로부터 추출된 정보를 결합하고, **베이즈 정리(Bayes Theorem)**를 이용하여 어떤 데이터가 특정 클래스에 속하는지 분류하는 알고리즘이다.

|장점|단점|
|---|---|
|단순하고 빠르며 매우 효과적이다|모든 속성은 동등하게 중요하고 독립적이라는 알려진 결함 가정에 의존한다|
|노이즈와 결측 데이터가 있어도 잘 수행한다|수치 속성으로 구성된 많은 데이터셋에 대해 이상적이지 않다|
|훈련에 대한 상대적으로 적은 예제가 필요하지만 매우 많은 예제도 잘 수행한다|추정된 확률은 예측된 범주보다 덜 신뢰적이다|
|예측에 대한 추정된 확률을 얻기 쉽다||
- 베이즈 정리
	$P(A \mid B) = \frac{P(B \mid A) \times P(A)}{P(B)}$
	- 어떤 사건 A가 주어졌을 때 사건 B가 일어날 확률을 갱신하는 방법을 제시합니다. 즉, 새로운 정보가 주어졌을 때, 해당 정보를 토대로 어떤 가설(사건)이나 모수를 추론·업데이트하는 데 활용됩니다.

---
## 머신러닝(machine)적 접근방법과 통계(statistics)적 접근방법

- 머신러닝적 접근방법: 모델의 **예측 성공률**을 높이는게 목적이다.  
	따라서 모델의 신뢰도나 정교한 가정보다는 다양한 피쳐를 사용하여 (오버피팅을 감안하더라도) 높은 예측률을 달성하고자 한다.

- 통계적 접근방법: 분포와 가정을 통해 **신뢰 가능하고 정교한** 모델을 만드는게 목적이다.
	따라서 모형을 복잡하지 않고 단순하게 만들고, 어떤 피쳐가 어떤 원인을 주는지 알 수 있도록 한다.


---
## 인공신경망(deep learning 이전)이 가지는 일반적인 문제점

![Pasted image 20250301145400.png](/img/user/images/Pasted%20image%2020250301145400.png)
- 선형적인 구조만을 이용해 회귀, 분류 등의 문제를 풀었기 떄문에 XOR문제와 같은 복잡한 문제는 해결하지 못했다.
	Sigmoid와 같은 비선형 함수를 선형모델 추가하면서 XOR과 같은 문제를 해결할 수 있게 되었다.

---
## Deep Learning의 혁신의 근간

- ImageNet 과 같은 **거대하고 높은 품질의 데이터셋**이 모두에게 공개되면서 딥러닝의 혁신적인 발전이 시작될 수 있었다. 현재는 더 다양한 태스크에 적합한 좋은 GLUE 같은 데이터들도 공개되어 더욱 딥러닝의 발전에 이바지하고 있다.

- 현재 좋은 성능을 내는 딥러닝 모델들은 모두 큰 규모의 모델들인데 **하드웨어의 발전**이 이를 가능하게 하였다.


---
## 서버가 많을 때 Random Forest가 유리한 이유

- 여러 Decision Tree를 이용해 앙상블하여 최종 결론을 내놓는 것이 Random Forest이다. 
	만약 서버의 수가 굉장히 많다면 각 서버마다 Decision Tree를 병렬적으로 처리하여 사용이 가능하다.
	이때 만약 인공 신경망을 사용한다면 서버 자체가 end to end 구조로 직렬적으로 구성되게 되므로 Random Forest가 유리하다.

---
## K-means

![Pasted image 20250301150314.png](/img/user/images/Pasted%20image%2020250301150314.png)

---
## L1, L2 정규화

- 정규화: 학습 데이터에 오버피팅 되지 않고 처음 보는 테스트 데이터에서도 좋은 성능을 내도록 만드는 것을 의미한다.
- 벡터의 Norm: 벡터의 크기를 나타낸다. L1에서는 절댓값으로, L2에서는 직선거리(제곱의 루트)로 나타낸다.

- **L1 정규화**
	- **공식**
		$\text{Loss}{\text{L1}} = \sum \text{(original loss)} + \lambda \sum{i}|w_i|$
		여기서 $\lambda$는 정규화 강도를 결정하는 하이퍼파라미터, $w_i$는 모델의 각 가중치를 의미
	
	- **특징**
		- **가중치의 절댓값을 최소화**
			가중치 $|w_i|$ 들을 더 작게 만들도록 압박(패널티)을 가합니다.
		- **희소성(Sparsity)을 유도**
			특정 가중치들이 **정확히 0**이 되는 효과가 있습니다. (일부 가중치는 완전히 0이 되고, 일부만 남아있게 됨)
			모델이 **Feature Selection**을 자동으로 수행하는 효과가 있습니다.
		- **가중치 업데이트에 불연속점이 생김**
			절댓값 함수$(|w|)$는 $w=0$에서 미분 불가능점이 존재하기 때문에, 경사하강법을 적용할 때 이에 대한 처리가 필요합니다(서브그래디언트 등).

- **L2 정규화**
	- 공식
		$\text{Loss}{\text{L2}} = \sum \text{(original loss)} + \lambda \sum{i}w_i^2$
	
	- **특징**
		- **가중치의 제곱합을 최소화**
			가중치 ($w_i^2$) 들을 더 작게 만들도록 압박합니다.
		- **가중치가 0에 가까워짐**
			모든 가중치가 조금씩 작아지는 경향이 있으며, 특정 가중치가 정확히 0이 되는 경우는 드뭅니다(희소성은 상대적으로 낮음).
		- **클 수록 더 강하게 패널티**
			가중치가 큰 항일수록 패널티 항에 큰 영향을 주므로, 큰 가중치가 더 빠르게 축소됩니다.
		- **미분이 연속적**
			$w^2$는 모든 $w$에 대해 미분 가능하므로 경사하강법 업데이트가 부드럽게 이뤄집니다.

- L1 정규화의 특성
	- L1 정규화를 사용하면 $w$ 벡터에서 많은 값이 0이 됩니다.
	- 이는 모델이 sparse(희소)해지도록 만듭니다.
	- 파라미터 중 많은 값이 0이 되므로, 모델을 저장하는 데 더 적은 메모리가 필요합니다.
	- 따라서 모델을 압축하는 데 유용합니다.

---
## Cross Validation이란?

- Cross Validation(교차검증): 학습(train) 데이터로 학습한 모델이, 학습에 사용되지 않은 **검증(validation) 데이터**에 대해 얼마나 잘 동작하는지를 평가하는 기법입니다. 
	일반화 성능을 높이기 위해 **검증용 데이터셋**을 따로 두고 학습 과정을 여러 번 반복해 평가하는 것이 핵심 아이디어입니다.

- 장단점
	-  **적은 데이터로도 안정적인 검증 가능**
		여러 번의 훈련-검증 과정을 통해, 소량의 데이터로도 모델 성능을 신뢰도 있게 평가할 수 있습니다.
	
	- **데이터 편중 최소화**
		모든 샘플이 학습에 사용될 기회를 얻고, 검증에도 고루 참여하게 되므로 특정 부분에만 편향되는 문제를 줄일 수 있습니다 (예: K-Fold Cross Validation).
	
	 - **더 일반화된 모델을 도출**
		 반복 학습-검증 결과를 종합해 모델 성능을 파악하고 하이퍼파라미터를 튜닝함으로써, 모델의 일반화 성능을 높일 수 있습니다.
	
	- **학습 시간 증가**
		폴드마다 모델을 새로 학습하고 검증해야 하므로, 단순 홀드아웃 방식에 비해 계산 비용이 커집니다.

- 홀드 아웃 교차검증
	일정한 비율의 Validation 데이터 셋 하나를 지정하여 검증 데이터 셋으로 사용하는 방법
	- Validation 데이터셋으로 지정된 부분의 데이터가 학습에 사용되지 않음
	- Validation 데이터셋에 편향됨

- K-fold 교차검증
	데이터를 k개의 fold로 나누어 그 중 하나의 fold를 validation 데이터 셋으로 사용하는 방법을 k번 반복하여 그 평균을 결과로 사용하는 방법
	1. train 데이터셋을 k개의 fold로 나누고, 그 중 하나를 validation 데이터셋으로 지정한다.
	2. validation 데이터셋을 제외한 나머지 폴드들을 train 데이터셋으로 사용하여 모델을 학습한다.
	3. 학습한 모델을 1번에서 지정해둔 validation 데이터셋으로 검증하고, 그 검증 결과를 저장해둔다.
	4. 모델을 초기화한 후, 기존 validation 데이터셋이 아닌 다른 fold를 validation 데이터셋으로 지정하고, 2번 과정부터 다시 수행한다.
	5. 모든 fold들이 한번씩 validation 데이터셋으로 사용된 후에는, 저장해둔 검증결과의 평균을 내어, 그것을 최종 validation 결과로 사용한다.
	
	- 랜덤하게 validation 데이터셋을 지정하게 되므로, 편향된 데이터로 이뤄진 폴드가 생성될 수 있다는 단점이 존재

- 계츨별 K-fold 교차검증
	랜덤하게 fold를 지정하는 것이 아닌, **각 클래스별 비율을 고려하여 fold를 구성하는 방법**이다.
	![Pasted image 20250301152742.png](/img/user/images/Pasted%20image%2020250301152742.png)

> [!NOTE]
    > **왜 test 데이터셋 만으로 검증하면 안될까?**  
    > 모든 train 데이터셋을 학습하고, test 데이터셋으로 검증한 결과를 확인한다고 하자. 개발자는 test 데이터셋 점수를 높이기 위해, test 데이터셋에 편향되도록 모델을 튜닝하게 될 것이다. 그러나 중요한 것은 test 데이터셋에 대한 정확도를 높이는 것 뿐만아니라, **모델의 일반적인 정확도를 높이는 것이다.** 어**떤 데이터가 들어와도 일정하게 높은 정확도를 보여주는 모델이 좋은 모델**이라 할 수 있으므로, validation 데이터셋과 test 데이터셋을 분리하여 검증하는 과정을 통해, 모델을 일반화시켜야 한다.

---
## XGBoost

- 트리 기반의 앙상블 학습에서 가장 각광받고 있는 알고리즘 중 하나이다.
- 예측모형의 **앙상블 방법론** 중 **부스팅** 계열에 속하는 알고리즘이다.

- 장점
	- 분류와 회귀영역에서 **뛰어난 예측 성능**을 발휘한다.
	- XGBoost는 병렬처리를 사용하여, GBM 대비 **빠른 수행시간**을 보인다.
	- **Regularization, Early Stopping** 기능을 통해 오버피팅을 방지할 수 있다.
	- Tree Pruning(가지치기) 제공한다. 미리 정해둔 max_depth까지만 split하고 pruning을 하고, 거꾸로 올라가면서 positive gain이 없는 노드를 삭제한다.
	- 자체적으로 결측치를 처리해준다.
	- 매 iteration마다 교차검증을 수행한다.

---
## 앙상블

![Pasted image 20250301153020.png](/img/user/images/Pasted%20image%2020250301153020.png)

- 앙상블: 여러개의 모델을 조합해서 그 결과를 뽑아 내는 방법이다. "정확도가 높은 강한 모델을 하나 사용하는 것보다, 정확도가 낮은 약한 모델을 여러개 조합 하는 방식의 정확도가 높다"는 개념에서 비롯한 방법이다. `Bagging`, `Boosting`, `Stacking` 등의 방법이 있다.
	
	- 배깅: 샘플을 여러번 뽑아(Bootstrap = 복원 랜덤 샘플링) 각 모델을 학습시켜 결과물을 집계(Aggregation)하는 방법
		카테고리 데이터는 투표 방식(Votinig)으로 결과를 집계하며, 연속형 데이터는 평균으로 집계한다.
		대표적인 기법에는 Random Forest, Hard Votin, Soft Voting 등이 있다.
	
	- 부스팅: 이전 모델의 **오답에 가중치를 높게 부여**하여 다음 모델을 학습하는 방법이다. **오답을 정답으로 맞추기 위해 오답에 더 집중하여 학습**시키기 떄문에 일반적으로 배깅에 비해 정확도가 높다.
		오답 부분을 반복적으로 학습하기 때문에 오버피팅 문제가 있고 outlier에 취약하고 속도가 느리다.
		대표적인 기법으로 XGBoost, AdaBoost, GradientBoost가 존재한다.
	
	- 스태킹: 여러 개별 모델이 예측한 결과값을 다시 학습 데이터셋으로 사용해서 모델을 만드는 방법이다.
		같은 데이터셋을 통해 예측한 결과를 기반으로 다시 학습하므로 오버피팅 문제점이 있다.
		Cross Validation 방식을 도입하여 이 문제를 해결할 수 있다. 데이터를 쪼개고 이들 중 일부만을 가지고 학습한 모델을 여러개 만들어, 그 결과들을 `메타 학습 데이터셋(meta train dataset)` 으로 사용하여 다시 학습하는 것이다. 이 방법은 많은 개별 모델의 결과를 결합하여 예측 성능을 높일 수 있다는 장점이 있다.
> [!NOTE]
    > **배깅 vs 부스팅**  
    > **배깅**은 랜덤 복원추출(부트스트랩)을 여러번 반복하여 모델을 **병렬적**으로 여러개 학습을 시킨 다음, 평균을 내는 방식이다. 반면, **부스팅**은 모든 데이터를 학습에 사용하되, 오답에 더 큰 가중치를 두어 다음 회차를 학습시키는 **순차적**인 방법이다

---
## 좋은 모델이란?

- 좋은 모델: **데이터의 패턴을 잘 학습한 모델**로서, **한번도 본적 없는 데이터에 대해 옳은 판단을 내리는 모델**이 좋은 모델이라고 할 수 있다.

---
## 50개의 작은 Decision Tree(배깅) vs 큰 Decision Tree

![Pasted image 20250301154139.png](/img/user/images/Pasted%20image%2020250301154139.png)
- 큰 Decision tree는 작은 편향(bias)와 큰 분산(variance)를 갖기 때문에 매우 깊은 Tree는 오버피팅이 발생하게 된다. 
- 배깅하는 방법은 편향은 그대로 유지하면서 여러 데이터 셋/여러 경우에 대해 학습하기 때문에 분산이 감소한다. 따라서 큰 Decision Tree보다 더 강건한 모델을 만들 수 있다.

---
## 분류 문제에서 Rogistic Regression vs Linear Regression

![images/Pasted image 202503011548291.png](/img/user/images/Pasted%20image%20202503011548291.png)
- Rogistic Regression
	**시그모이드 함수(sigmoid function)** 를 통해 선형함수를 0과 1 사이의 함수로 바꾼 것이며, S자 형태를 보인다. 시그모이드 함수의 정의는 $S(x) = \frac{1}{ 1+ e^{-x}} = \frac{e^x}{e^x + 1}$와 같다.

- Linear Regression
	어떤 입력값이 들어오느냐에 따라 **0과 1 사이의 범위를 벗어나기도** 한다.