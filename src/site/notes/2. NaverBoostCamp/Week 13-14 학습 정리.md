---
{"dg-publish":true,"permalink":"/2-naver-boost-camp/week-13-14/","created":"2025-02-26T15:44:18.920+09:00","updated":"2025-03-12T14:01:15.019+09:00"}
---

# Data-centric AI와 DMOPs: 좋은 데이터가 AI 성능의 핵심이다

**AI 시스템은 모델(알고리즘)과 데이터로 구성**됩니다. 전통적인 AI 교육이나 연구에서는 고정된 데이터셋과 평가 방식을 바탕으로 모델의 성능을 향상시키는 데 초점을 맞춥니다. 그러나 실제 현업에서는 "영수증 데이터 수집 자동화"와 같이 요구사항만 주어지는 경우가 많으며, 전체 AI 개발 업무 중 실제 서비스에 적용되는 부분은 데이터셋 준비와 정제에 큰 비중(~80%)을 차지합니다.  
이러한 맥락에서 **Data-centric AI**는 좋은 품질의 데이터를 확보하고 관리하는 것이 AI 성능 향상의 결정적인 요인입니다.
![Pasted image 20250312134527.png](/img/user/Pasted%20image%2020250312134527.png)

---

## 1. Data-centric AI의 중요성

- **모델 vs. 데이터:**  
    연구에서는 모델링이 주된 관심사일 수 있으나, 실제 서비스에서는 데이터 준비와 정제가 모델 변경보다 훨씬 큰 비용과 영향을 미칩니다.
    
- **서비스 출시 이후:**  
    일단 모델이 배포되면, 모델 자체를 변경하는 데 드는 비용이 매우 커지므로, 성능 개선은 주로 데이터를 정제하거나 추가하는 방향으로 진행됩니다.
    

따라서 AI 시스템의 성능을 극대화하려면, 높은 품질의 데이터를 확보하는 것이 필수적입니다.

---

## 2. DMOPs (Data Management OPerations & Recipes)

**DMOPs**는 양질의 데이터를 확보하기 위한 방법론, 도구, 프레임워크를 포괄하는 개념입니다. DMOPs의 주요 구성 요소는 다음과 같습니다.

- **Data Annotation Tool:**  
    데이터를 효율적으로 주석 처리할 수 있는 도구
- **Data Software Tool:**  
    데이터 전처리 및 정제, 분석에 도움을 주는 소프트웨어
- **Data Labeling Tool:**  
    데이터에 올바른 라벨을 부여하기 위한 도구
- **Crowd Sourcing:**  
    언제 어디서나 누구나 온라인 플랫폼을 통해 데이터 라벨링 작업에 참여할 수 있도록 하는 방식

이와 같은 DMOPs 접근법을 통해, 좋은 데이터를 만드는 작업을 체계적이고 효율적으로 수행할 수 있습니다.

---

## 3. 좋은 데이터를 만들기 위한 라벨링 가이드

좋은 데이터를 확보하는 것은 쉽지 않은 문제입니다. 데이터 수집량이 충분하지 않거나, 라벨링 작업의 명확한 정답이 없고, 비용도 많이 소요되기 때문입니다. 이에 따라, 일관된 고품질 데이터를 만들기 위해 **명확한 라벨링 가이드**를 작성하는 것이 중요합니다.

라벨링 가이드는 다음과 같은 요소를 포함해야 합니다.

- **일관성:**  
    모든 데이터에 대해 동일한 기준과 절차로 라벨링하여, 모델이 혼란 없이 학습할 수 있도록 합니다.
    
- **특이 케이스 포함:**  
    일반적인 사례뿐 아니라, 예외적인 케이스들도 포함하여 모델이 다양한 상황에 잘 대응할 수 있도록 합니다.
    
- **High Quality:**  
    데이터 자체의 품질이 높아야 하며, 노이즈나 오류가 최소화되어야 합니다.
    
- **균형 잡힌 분포:**  
    데이터가 특정 클래스나 특성에 치우치지 않도록 골고루 분포되어 있어야 합니다. 편향된 데이터는 모델의 일반화 능력을 저하시킬 수 있습니다.
    

---

## 결론

실제 AI 서비스 개발에서는 모델링보다 데이터 준비와 정제가 훨씬 큰 비중을 차지합니다. 좋은 데이터를 확보하고, 이를 체계적으로 관리하기 위한 DMOPs와 명확한 라벨링 가이드를 통해, 모델 성능을 안정적으로 개선할 수 있습니다. Data-centric AI 접근법은 AI의 성능 향상을 위한 핵심 전략이며, 성공적인 서비스 운영의 필수 요소입니다.

---
# OCR과 최신 기술: 이미지에서 텍스트를 읽어내는 혁신

**OCR (Optical Character Recognition)** 은 이미지 내의 글자를 인식하여 텍스트로 변환하는 기술입니다. 단순히 종이에 적힌 글자를 읽는 것을 넘어, 길거리 간판이나 다양한 자연 환경에서의 텍스트도 인식할 수 있는 **STR (Scene Text Recognition)** 의 발전으로, OCR은 실생활에 폭넓게 적용되고 있습니다.

---

## 1. OCR의 핵심 과제

OCR은 단순한 객체 검출(object detection)과 달리, **텍스트 영역의 위치를 검출한 후** 그 영역 내의 글자를 전사하는 두 단계로 구성됩니다. OCR만의 특이점은 다음과 같습니다.

- **밀도가 높음:** 텍스트 영역에 글자가 촘촘하게 배열됨
- **극단적 종횡비:** 가로, 세로 길이가 크게 차이날 수 있음
- **특이 모양:** 구겨지거나 휘어진 텍스트, 세로쓰기 등 다양한 형태

---

## 2. 텍스트 영역 표현법

OCR 시스템은 텍스트 영역을 다양한 방식으로 표현할 수 있습니다.

- **RECT:**
    - `(x1, y1, height, width)` 또는 `(x1, y1, x2, y2)`
- **RBOX:**
    - `(x1, y1, height, width, θ)` 또는 `(x1, y1, x2, y2, θ)`
    - 회전 정보를 포함하여 기울어진 텍스트 영역을 표현
- **QUAD:**
    - Bounding box의 좌상단부터 시계 방향으로 4개의 좌표: `(x1, y1, x2, y2, x3, y3, x4, y4)`
- **Polygon:**
    - 다각형 형태로, 임의의 N개의 점으로 영역을 정의

---

## 3. OCR의 주요 모듈

효과적인 OCR 시스템은 다음 네 가지 모듈로 구성됩니다.

1. **Text Detector:**
    - 이미지 내 텍스트 박스의 위치를 검출합니다.
2. **Text Recognizer:**
    - 검출된 텍스트 영역에서 실제 글자를 인식하고 전사합니다.
    - 이는 Computer Vision과 NLP 기술의 결합 영역입니다.
3. **Serializer:**
    - 인식된 텍스트를 자연어 처리하기 쉽게 정렬합니다.
    - 단락을 묶고 좌상단부터 우하단으로 정렬하며, 금칙어 처리나 요약 등의 후처리도 가능합니다.
4. **Text Parser:**
    - 정렬된 텍스트를 미리 정의된 키(예: 이름, 전화번호, 이메일 등)와 매핑합니다.
    - 이 과정에서는 BIO tagging을 활용하여 단어별 시작(B), 내부(I), 외부(O)을 태깅합니다.

---

## 4. 최신 OCR 모델

최근에는 Transformer와 같은 최신 모델 구조를 적용한 OCR 시스템이 등장했습니다.

### TrOCR

- **구성:**
    - 사전 학습된 Transformer 모델을 Encoder로, 사전 학습된 Language Model을 Decoder로 사용
- **특징:**
    - 복잡한 전처리나 후처리 없이 우수한 성능을 보임

### DTrOCR

- **구성:**
    - 이미지 Encoder 없이 이미지 임베딩을 바로 더 큰 Text Decoder에 넣어 처리
- **특징:**
    - 파라미터 수는 적으면서도, 더 큰 사전 학습 데이터셋을 활용하여 성능 향상

### MATRn (Multi-modAl Text Recognition Network)

- **문제 해결:**
    - 글자가 가려지거나 잘려 나온 이미지, 낮은 해상도 등에서 정확도 향상을 위해 multi-modal 접근 적용
- **구성:**
    - **Feature Extractor:** 기본 특징 추출
    - **Multi-modal Feature Enhancement:** 다양한 모달리티를 고려한 특징 강화
    - **Output Fusion:** 여러 출력을 결합해 최종 인식 결과 생성

---

## 결론

OCR은 단순한 텍스트 인식을 넘어, 다양한 환경에서 텍스트를 효과적으로 읽어내기 위한 기술입니다.

- **텍스트 영역 표현법:** RECT, RBOX, QUAD, Polygon 등 다양한 방식으로 표현 가능
- **모듈 구성:** Text Detector, Recognizer, Serializer, Parser를 통해 텍스트를 인식하고 후처리
- **최신 모델:** TrOCR, DTrOCR, MATRn 등 Transformer 기반 모델이 높은 성능을 달성하고 있습니다.

---
# 서비스 향 AI 모델 개발 시 데이터 점검과 OCR 데이터셋 확보 전략

AI 성능은 좋은 모델뿐 아니라 양질의 데이터를 확보하는 데 크게 좌우됩니다. 특히 실제 서비스에 적용되는 AI 모델을 개발할 때는, 단순히 모델 아키텍처를 개선하는 것보다 데이터의 품질을 체계적으로 관리하는 것이 더욱 중요합니다.  
서비스향 AI 모델 개발 시, 아래와 같은 질문들을 통해 현재 상황을 진단하고, 최신 모델의 한계와 일반적인 경우, 그리고 outlier 케이스를 파악할 수 있습니다.

- **몇 장의 데이터를 학습시켰을 때 어느 정도 성능이 나오는지?**
- **어떤 경우가 일반적이고, 어떤 경우가 outlier로 나타나는지?**
- **최신 모델이 가지는 한계는 무엇인지?**

이러한 질문들에 답을 얻기 위해, 공개된 데이터셋과 최신 모델을 활용해 실험해보는 것이 효과적입니다.

---

## 1. 서비스향 AI 모델 개발 시 점검해야 할 사항

### 1-1. 성능 기준 및 데이터 분포

- **학습 데이터 양 대비 성능:**  
    충분한 데이터로 학습 시 일반적인 성능과 학습 곡선을 확인하여, 어느 시점에 성능이 포화되는지 파악합니다.
- **일반 케이스 vs Outlier:**  
    전체 데이터 분포에서 흔히 발생하는 경우와, 특이하거나 드문 케이스를 구분하여 평가합니다.
- **최신 모델의 한계:**  
    최신 모델들이 해결하지 못하는 문제점(예: 복잡한 환경, 노이즈, 다양한 폰트 및 스타일 등)을 실험을 통해 식별합니다.

### 1-2. 데이터 중심 AI(Data-centric AI) 접근법

- **데이터 정제와 보강:**  
    서비스 출시 이후 모델 변경 비용이 크므로, 성능 개선은 주로 데이터를 정제, 추가하는 방향으로 이루어집니다.
- **데이터의 다양성:**  
    다양한 상황을 반영한 데이터를 확보하여, 모델이 실제 서비스 환경에서 안정적인 성능을 발휘할 수 있도록 합니다.

---

## 2. OCR 데이터셋 확보 방법

OCR (Optical Character Recognition)은 이미지 내의 텍스트를 검출하고 인식하는 태스크입니다. OCR 데이터셋은 일반 텍스트 데이터셋과 달리, 글자의 모양, 언어, 배경 등 다양한 요소를 포함합니다. 다음은 OCR 데이터셋을 확보할 수 있는 주요 방법입니다.

- **Kaggle:**  
    다양한 OCR 관련 경진대회와 데이터셋이 공개되어 있습니다.
- **RRC (Robust Reading Challenges):**  
    2년마다 열리는 OCR 전문 대회로, 최신 OCR 데이터셋과 benchmark가 제공됩니다.
- **논문:**  
    Arxiv, CVPR, ICCV, AAAI, ICDAR 등에서 공개된 데이터셋을 참고할 수 있습니다.
- **Google Datasearch:**  
    구글에서 제공하는 데이터셋 검색 서비스를 통해 다양한 공개 데이터셋을 찾을 수 있습니다.
- **Zenodo.org:**  
    연구 커뮤니티에서 공유하는 다양한 데이터셋이 공개되어 있습니다.
- **Datatang:**  
    유료 데이터 구매 사이트로, 특화된 OCR 데이터셋을 확보할 수 있습니다.

---

## 3. OCR 데이터셋 파악 시 고려해야 할 특성

OCR 데이터셋을 분석할 때는 다음과 같은 요소들을 주의 깊게 살펴봐야 합니다.

- **언어:**  
    데이터셋에 포함된 언어와 스크립트 종류 (영어, 한글, 일본어 등)
- **용도:**
    - **Detection:** 텍스트 영역 검출을 위한 데이터
    - **Recognition:** 텍스트 인식을 위한 데이터
    - **End-to-End:** 텍스트 검출과 인식을 모두 포함하는 데이터셋
- **데이터 수량:**  
    충분한 양의 데이터를 확보하여 다양한 케이스를 포괄할 수 있는지
- **라이센스 종류:**  
    데이터 사용에 제약이 있는지, 상업적 활용이 가능한지 등 라이센스 조건 확인
- **데이터 저장 포맷:**  
    이미지 파일 형식(JPEG, PNG, TIFF 등), 어노테이션 형식(XML, JSON 등)
- **특이사항:**
    - 글자 밀도, 종횡비
    - 텍스트의 형태(구겨짐, 휘어짐, 세로쓰기 등)
    - 배경의 복잡도와 노이즈 수준

---

## 결론

서비스 향 AI 모델을 개발할 때는 모델 성능뿐만 아니라 데이터의 품질과 다양성이 결정적인 역할을 합니다.

- **모델 점검:** 학습 데이터 양, 일반 케이스와 outlier 구분, 최신 모델의 한계를 파악합니다.
- **데이터셋 확보:** Kaggle, RRC, 논문, Google Datasearch, Zenodo, Datatang 등을 활용하여 다양한 OCR 데이터셋을 수집합니다.
- **데이터셋 특성 파악:** 언어, 용도, 데이터 수량, 라이센스, 저장 포맷, 특이사항 등 세밀하게 검토하여, 데이터 중심 AI 접근법을 효과적으로 적용합니다.

이러한 접근법은 서비스에 적용되는 AI 모델의 성능을 극대화하고, 향후 발생할 수 있는 문제에 대비할 수 있도록 도와줄 것입니다.

---
# OCR 성능 평가: Bounding Box 관점에서의 평가 Metrics

**OCR (Optical Character Recognition) 시스템은 이미지 내 텍스트 영역을 정확하게 검출하고, 그 영역 내의 글자를 인식하는 것이 핵심**입니다. 여기서는 OCR의 텍스트 박스(경계 상자, bounding box) 검출 성능을 평가하는 방법에 대해 다룹니다.  
일반적인 object detection 평가 지표와 유사하지만, **OCR 특성에 맞춰 몇 가지 추가 지표와 페널티가 적용**됩니다.

---

## 1. Area Recall & Area Precision
![Pasted image 20250312134754.png](/img/user/Pasted%20image%2020250312134754.png)
### Area Recall

- **정의:**  
    Ground Truth (GT) bbox에 대해, 예측한 predicted bbox와 겹치는 영역의 비율입니다.
- **수식:** Area Recall=두 bbox의 교집합 영역Ground Truth bbox의 영역\text{Area Recall} = \frac{\text{두 bbox의 교집합 영역}}{\text{Ground Truth bbox의 영역}}
- **해석:**  
    실제 텍스트 영역 중 얼마나 예측 bbox가 포함되었는지를 나타내며, 누락된 영역이 적을수록 Recall이 높습니다.

### Area Precision

- **정의:**  
    Predicted bbox의 영역에 대해, 실제로 GT와 겹치는 부분의 비율입니다.
- **수식:** Area Precision=두 bbox의 교집합 영역Predicted bbox의 영역\text{Area Precision} = \frac{\text{두 bbox의 교집합 영역}}{\text{Predicted bbox의 영역}}
- **해석:**  
    예측한 영역이 얼마나 정확하게 실제 텍스트 영역에 일치하는지를 나타내며, 불필요한 영역이 적을수록 Precision이 높습니다.

### 비교 요약

|구분|Area Precision|Area Recall|
|---|---|---|
|**기준**|Predicted bbox|Ground Truth bbox|
|**평가 질문**|예측한 bbox가 얼마나 정확한가?|실제 객체를 얼마나 잘 포함했는가?|
|**의미**|예측한 영역 중 GT와 일치하는 비율|GT 영역 중 예측 bbox에 포함된 비율|

---

## 2. Match 유형
![Pasted image 20250312134807.png](/img/user/Pasted%20image%2020250312134807.png)
OCR 태스크에서는 한 이미지 내에 여러 텍스트가 존재하기 때문에, GT bbox와 predicted bbox 간의 매칭이 중요합니다.

- **One-to-One Match:**  
    GT와 predicted bbox가 1:1로 정확하게 매칭된 경우.
    
- **One-to-Many Match:**  
    하나의 GT bbox에 대해 여러 predicted bbox가 분할되어 매칭된 경우.
    
    - 이 경우, 페널티로 보통 0.8을 곱해 성능 점수를 낮춥니다.
- **Many-to-One Match:**  
    여러 GT bbox가 하나의 predicted bbox로 합쳐져 매칭된 경우.
    
    - 일반적으로 페널티 없이 처리합니다.

---

## 3. 주요 평가 Metric

### 3-1. DetEval

DetEval은 OCR에서 bbox 검출 성능을 평가하기 위한 종합 Metric입니다.

1. **매칭:**  
    각 GT bbox와 predicted bbox 쌍을 매칭하고, 각 쌍에 대해 Area Recall과 Area Precision을 계산합니다.
	![Pasted image 20250312134832.png](/img/user/Pasted%20image%2020250312134832.png)
    
2. **Binary 평가:**  
    각 쌍에 대해 다음 조건을 만족하면 1, 그렇지 않으면 0으로 점수를 부여합니다.
    
    - Area Recall ≥ 0.8
    - Area Precision ≥ 0.4
	![Pasted image 20250312134846.png](/img/user/Pasted%20image%2020250312134846.png)
	
3. **매칭 유형 고려:**
    
    - One-to-One: 그대로 1점
    - One-to-Many: 페널티 0.8 적용
    - Many-to-One: 페널티 없이 1점 적용
	
	![Pasted image 20250312134859.png](/img/user/Pasted%20image%2020250312134859.png)
	
4. **최종 점수:**  
    GT 기준 평균(Recall)과 예측 기준 평균(Precision)을 각각 계산하고, F1 score로 종합합니다.
    
    $\text{F1 score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$
    

### 3-2. IoU (Intersection over Union)

- **정의:**  
    GT bbox와 predicted bbox의 겹치는 영역의 비율로, IoU=교집합 영역합집합 영역
    $\text{IoU} = \frac{\text{교집합 영역}}{\text{합집합 영역}}$
    
- **참고:**  
    Object Detection 평가의 기본 지표입니다.

### 3-3. TIoU (Tightness-aware IoU)

TIoU는 단순 IoU와 달리, GT bbox에 비해 예측 bbox가 부족하거나 초과한 영역에 대해 페널티를 부여합니다.

- **TIoU Recall:**
    
    $\text{TIoU}_{\text{recall}} = \text{IoU} \times (1 - \text{penalty})$
    
    여기서 penalty는 GT bbox에 비해 부족한 영역의 크기에 비례하여 계산됩니다.
    
- **TIoU Precision:**
    
    $\text{TIoU}_{\text{precision}} = \text{IoU} \times (1 - \text{penalty})$
    
    여기서 penalty는 predicted bbox가 GT bbox보다 초과된 영역의 크기에 비례합니다.
    
- **최종 TIoU:**  
    TIoU Recall과 TIoU Precision의 조화 평균:
    
    $\text{TIoU} = 2 \times \frac{\text{TIoU}_{\text{recall}} \times \text{TIoU}_{\text{precision}}}{\text{TIoU}_{\text{recall}} + \text{TIoU}_{\text{precision}}}$
    

> **주의:**  
> TIoU는 GT가 golden answer라는 가정이 필요하며, 단순 넓이 기반 페널티로 인해 일부 글자 누락 같은 세밀한 오류는 반영하지 못할 수 있습니다.

### 3-4. CLEval (Character-Level Evaluation)

CLEval은 OCR의 detection뿐 아니라 recognition 성능도 평가하는 문자 단위 지표입니다.

1. **Pseudo Character Centers (PCC):**  
    각 글자의 중심을 계산하여 평가합니다.
    
2. **매칭 및 점수 계산:**
    
    - 각 GT bbox 내의 PCC와 predicted bbox 내의 PCC를 매칭하여, 
	    $\text{Recall} = \frac{\sum(\text{Correct PCC} - \text{Granual Penalty})}{\text{GT 내 PCC 총 개수}}$
    - Precision도 비슷한 방식으로 계산하며, 여러 predicted bbox에 속하는 PCC는 해당 bbox 수로 나누어 계산합니다.
	    $\text{Precision} = \frac{\sum \text{CorrectNum} - \sum \text{GranualPenalty}}{\sum \text{TotalNum}}$
3. **최종 CLEval:**  
    Recall과 Precision의 조화 평균:
    
    $\text{CLEval} = \frac{2 \times (\text{Recall} \times \text{Precision})}{\text{Recall} + \text{Precision}}$
    

---

## 4. 예시

아래 세 가지 케이스에 대해 각 Metric의 예시 점수를 살펴보겠습니다.
![Pasted image 20250312135358.png](/img/user/Pasted%20image%2020250312135358.png)
### (1) 정답 케이스

|Metric|Score|
|---|---|
|DetEval|1.0|
|IoU|1.0|
|TIoU|0.9|
|CLEval|8/9|

### (2) 정답 케이스

|Metric|Score|
|---|---|
|DetEval|0.8|
|IoU|0.0|
|TIoU|-|
|CLEval|0.9|

### (3) 정답 케이스

|Metric|Score|
|---|---|
|DetEval|1.0|
|IoU|0.0|
|TIoU|-|
|CLEval|0.93|

> **해석:**  
> 각 케이스에서 DetEval, IoU, TIoU, CLEval은 OCR의 bounding box 검출 성능을 다른 관점에서 평가합니다.
> 
> - DetEval은 binary 기준과 매칭 페널티를 반영하며,
> - IoU는 단순 겹침 비율,
> - TIoU는 여분 혹은 부족한 영역에 대해 페널티를 주고,
> - CLEval은 문자 단위 세밀 평가를 제공합니다.

---

## 결론

OCR 시스템의 bounding box 성능 평가는 단순한 IoU 이상의 다양한 지표를 통해 이루어집니다.

- **Area Recall/Precision:** GT와 예측 bbox의 겹침 비율로 기본적인 성능 확인
- **DetEval:** 매칭 및 binary 기준 평가로 실제 서비스 적용 가능성을 판단
- **TIoU:** 예측 bbox의 타이트함(정밀도)을 반영하여 페널티 부여
- **CLEval:** 문자 단위 평가로, detection과 recognition 모두를 반영

이러한 다양한 지표를 활용하면 OCR 모델의 bbox 검출 성능을 다각도로 평가하고, 개선 방향을 모색할 수 있습니다.

---
# 데이터 Annotation: 효율적인 이미지 라벨링 도구 소개

**Annotation** 은 이미지, 비디오, 텍스트 등 시각 데이터를 머신러닝 모델 학습에 활용할 수 있도록 라벨(정보)를 추가하는 작업입니다. 예를 들어, object detection 문제에서는 이미지 내 객체의 위치를 나타내는 bounding box와 해당 객체의 클래스(label)를 지정해야 합니다. 오늘은 이러한 annotation 작업을 보다 수월하게 진행할 수 있는 대표적인 도구들을 살펴보겠습니다.

---

## 1. 주요 Annotation 도구

### LabelMe

- **개요:**  
    MIT CSAIL에서 공개한 이미지 데이터 annotation 도구를 참고하여 만들어진 오픈소스 소프트웨어입니다.
- **특징:**
    - 이미지를 열어 bounding box와 라벨을 직접 작성할 수 있으며, 결과는 JSON 파일로 저장됩니다.
    - Python으로 작성되어 있어, 필요에 따라 기능을 추가하거나 커스터마이징할 수 있습니다.
- **단점:**
    - 다수 사용자가 동시에 작업하기 어려워 협업에 한계가 있습니다.
    - object나 image에 대한 추가 속성 부여 기능은 제공되지 않습니다.

---

### CVAT

- **개요:**  
    Intel OpenVINO 팀에서 제작한 공개 computer vision 데이터 제작 도구입니다.
- **특징:**
    - 웹 환경에서 동작하여 Chrome 브라우저 등에서 사용이 용이합니다.
    - 작업 프로세스: 새로운 task 생성 → 라벨 등록 → 이미지 업로드 → bbox 및 라벨링 작업 후 데이터셋 Export.
    - 단축키(예: n: bbox 생성, ctrl+s: 저장, delete: bbox 삭제 등)를 지원하여 작업 효율을 높입니다.
    - assignee와 reviewer를 지정할 수 있어, 다수 사용자 간 공동 작업이 가능합니다.
    - automatic annotation 기능을 제공해 일부 작업을 자동화할 수 있습니다.
- **단점:**
    - model inference가 상대적으로 느린 편입니다.
    - object나 image에 대한 추가 속성 부여 기능은 제공되지 않습니다.

---

### Hasty Labeling Tool

- **개요:**  
    주로 유료 서비스로 제공되며, free credit 소진 이후 유료로 전환됩니다.
- **특징:**
    - semi-automated annotation 기능을 제공하여, 일부 작업을 자동화할 수 있습니다.
    - assignee와 reviewer 기능을 통해 멀티 유저 협업 환경을 지원합니다.
- **단점:**
    - 도구 커스터마이징이 제한적입니다.

---

### LabelImg

- **개요:**  
    Pascal VOC 포맷의 XML 파일을 기반으로 하는 간단한 이미지 라벨링 도구입니다.
- **특징:**
    - 지정된 폴더 내 이미지들을 불러와, 이미지 위에 표시되는 bounding box와 라벨을 확인하며 작업할 수 있습니다.
    - 직관적인 인터페이스로 빠르게 annotation 작업을 진행할 수 있습니다.
- **단점:**
    - 상대적으로 단순한 기능을 제공하며, 협업 기능이나 자동화 기능은 미흡합니다.

---

## 2. 선택 시 고려 사항

- **협업 필요성:**  
    다수 사용자가 동시에 작업해야 하는 경우, CVAT나 Hasty Labeling Tool처럼 협업 기능을 지원하는 도구가 유리합니다.
- **자동화 수준:**  
    작업량이 많아 자동화가 필요한 경우, automatic 또는 semi-automatic annotation 기능이 있는 도구를 선택하는 것이 좋습니다.
- **커스터마이징:**  
    특정 요구 사항에 맞게 기능을 확장하거나 수정하고자 할 때는, Python 기반의 오픈소스 도구(LabelMe, LabelImg)가 유리할 수 있습니다.
- **데이터 포맷 및 속성 부여:**  
    작업하려는 데이터의 특성과 후처리 요구 사항에 맞춰, 어떤 포맷(XML, JSON 등)과 속성 부여가 가능한지를 고려해야 합니다.

---

## 결론

이미지 annotation은 AI 모델 학습의 품질과 성능을 좌우하는 중요한 전처리 단계입니다.

- **LabelMe** 는 Python 기반 커스터마이징이 용이한 도구이고,
- **CVAT** 는 웹 기반 협업과 단축키, 자동화 기능이 뛰어나며,
- **Hasty Labeling Tool** 은 semi-automated 기능을 제공하지만 커스터마이징은 제한적이며,
- **LabelImg** 는 단순하지만 빠른 작업 환경을 제공합니다.

각 도구의 장단점을 고려하여 프로젝트와 팀의 요구에 맞는 annotation 도구를 선택하면, 고품질의 데이터를 확보하는 데 큰 도움이 될 것입니다.

---
# Data-Centric AI: 좋은 데이터를 위한 Annotation 가이드라인과 일관성 평가

AI 모델의 성능 향상에서 데이터의 품질은 모델 자체보다 훨씬 중요한 역할을 합니다. Data-Centric AI 접근법에서는 **편향되지 않고 골고루 분포된**, 그리고 **일정한 기준**에 따라 라벨링된 데이터를 확보하는 것이 핵심입니다. 이를 위해 명확하고 간결한 **Annotation 가이드라인**을 마련하는 것이 필수적입니다.

이 포스트에서는 데이터 annotation 가이드라인에 포함되어야 할 내용과, annotation의 품질을 평가하기 위한 Inter-Annotator Agreement (IAA) 지표들(Cohen's Kappa, Fleiss' Kappa, Krippendorff's Alpha, 그리고 최근 KS test)을 살펴봅니다.

---

## 1. Annotation 가이드라인 작성

좋은 데이터를 구축하기 위한 Annotation 가이드라인은 데이터 확보 및 라벨링 과정의 표준화를 위해 작성된 문서입니다. 가이드라인은 아래와 같은 요소들을 포함해야 하며, 해석의 여지가 없도록 **간결하고 명확하게** 작성되어야 합니다.

### 필수 포함 내용

- **데이터 구축의 목적:**
    
    - 데이터 수집 및 annotation 작업의 최종 목표와 활용 방안을 명시
- **라벨링 대상 이미지 소개:**
    
    - 어떤 종류의 이미지(예: 영수증, 명함, 길거리 간판 등)를 대상으로 하는지 설명
- **특이 케이스 대처 방법:**
    
    - 예외적 상황이나 모호한 사례에 대해 어떻게 처리할 것인지 지침 제공
- **기본적인 용어 정의:**
    
    - "bounding box", "전사", "태그" 등 자주 사용되는 용어에 대한 정의
    - 모든 annotator가 동일한 기준으로 이해할 수 있도록
- **Annotation 규칙:**
    
    - 어떤 방식으로 annotation할 것인지 (예: bbox 작업 방식, 라벨링 기준 등)
    - 최종 데이터 형식(예: JSON, XML, Pascal VOC 등)
- **작업불가 이미지 정의 (나쁜 데이터 기준):**
    
    - 불량 이미지, 너무 흐리거나 왜곡된 이미지 등 annotation 작업에서 제외할 대상을 명시
- **작업불가 영역 정의:**
    
    - illegibility = True와 같이, 읽기 어려운 영역에 대한 기준 제시
- **BBOX 작업 방식 정의:**
    
    - 좌표 표기법 (예: (x1, y1, x2, y2) 등) 및 작업 시 주의 사항
- **최종 데이터 포맷:**
    
    - annotation 결과가 저장될 형식과 구조를 명확히 기술

이와 같이 명확한 가이드라인을 작성하면, annotator 간에 일관된 작업이 이루어지고, 좋은 품질의 데이터셋 구축에 큰 도움이 됩니다.

---

## 2. Inter-Annotator Agreement (IAA)

Annotation 데이터의 품질은 annotator들이 얼마나 일관되게 라벨링하는지에 달려 있습니다. **Inter-Annotator Agreement (IAA)** 는 여러 annotator가 생성한 라벨의 일치도를 평가하는 지표로, 대표적으로 Cohen's Kappa, Fleiss' Kappa, Krippendorff's Alpha 등이 사용됩니다.

### 2-1. Cohen's Kappa

- **정의:**  
    두 명의 annotator가 라벨링한 결과가 우연히 일치할 가능성을 제거하고, 실제 일치 정도를 측정합니다.
    
- **수식:**
    
	    $\kappa = \frac{P_{\text{observed}} - P_{\text{expected}}}{1 - P_{\text{expected}}}$
    - $P_{\text{observed}}$: 관측된 일치도
    - $P_{\text{expected}}$: 우연히 일치할 확률
- **해석:**
    
    - κ<0\kappa < 0: Poor
    - 0.0 - 0.2: Slight
    - 0.2 - 0.4: Fair
    - 0.4 - 0.6: Moderate
    - 0.6 - 0.8: Substantial
    - 0.8 - 1.0: Almost Perfect  
        일반적으로 0.6 이상이면 일치도가 보통 이상이라고 평가합니다.

---

### 2-2. Fleiss' Kappa

- **정의:**  
    세 명 이상의 annotator가 생성한 라벨의 일관성을 평가할 때 사용합니다.
    
- **수식:**
    
    $\kappa = \frac{\bar{P}_{\text{observed}} - \bar{P}_{\text{expected}}}{1 - \bar{P}_{\text{expected}}}$
    - $\bar{P}_{\text{observed}}$: 모든 항목에 대한 평균 일치도
    - $\bar{P}_{\text{expected}}$: 무작위 선택 시 기대되는 평균 일치도
- **해석:**  
    Cohen's Kappa와 유사한 기준으로 평가합니다.
    

---

### 2-3. Krippendorff's Alpha

- **정의:**  
    두 명 이상의 annotator가 생성한 라벨의 일관성을 평가하는 지표로, 다양한 데이터 유형(명목, 순서, 간격 등)에 적용 가능합니다.
    
- **수식:**
    
    $\alpha = 1 - \frac{\hat{D}_{\text{observed}}}{\hat{D}_{\text{expected}}}$
    - $\hat{D}_{\text{observed}}$: 관측된 차이(불일치)
    - $\hat{D}{\text{expected}}$: 우연히 발생할 것으로 기대되는 차이
- **해석:**
    
    - α<0.667\alpha < 0.667: 데이터를 재검토하거나 폐기
    - 0.667 - 0.8: 잠정적인 결론
    - 0.8 이상: 신뢰할 수 있는 데이터  
        단, 복잡한 태스크에 적용 시 분산을 포착하지 못해 한계가 있을 수 있습니다.

---

### 2-4. Kolmogorov-Smirnov (KS) Test

최근에는 KS Test를 활용하여 annotator가 생성한 라벨 데이터의 분포까지 고려한 일치도를 평가하는 연구도 진행되고 있습니다. KS Test는 두 데이터 분포 간의 차이를 통계적으로 검정할 수 있어, 단순 평균 일치도보다 더 세밀한 평가가 가능합니다.

---

## 결론

좋은 데이터는 AI 모델 성능 향상의 핵심입니다.

- **Annotation 가이드라인**을 통해 일관되고 정확한 라벨링 작업을 수행하면, 데이터의 품질을 크게 향상시킬 수 있습니다.
- **Inter-Annotator Agreement (IAA)** 평가 지표(Cohen's Kappa, Fleiss' Kappa, Krippendorff's Alpha, KS Test)를 통해 데이터 annotation의 일관성을 정량적으로 측정하고, 필요시 개선할 수 있습니다.

이러한 접근법은 데이터 중심 AI 전략에서 좋은 데이터를 확보하여 최종적으로 AI 모델의 성능을 극대화하는 데 중요한 역할을 합니다.

---
# OCR 데이터 가공 전략: Augmentation과 합성 데이터 제작

OCR (Optical Character Recognition)은 이미지 내 글자를 검출하고 인식하는 중요한 태스크입니다. 하지만 **실제 서비스 환경에서는 데이터셋이 편향되고 양이 부족한 경우가 많습니다.** Data-centric AI 접근법에서 강조하는 것처럼, 좋은 데이터—즉, 편향되지 않고 골고루 분포하며 일정한 기준으로 라벨링된 데이터—가 AI 성능 향상의 핵심입니다.  
이번 포스팅에서는 OCR 태스크에서 데이터를 가공하는 두 가지 주요 전략, **데이터 증강 (Augmentation)** 과 **합성 데이터 제작** 에 대해 살펴보겠습니다.

---

## 1. 데이터 증강 (Augmentation)
![Pasted image 20250312140112.png](/img/user/Pasted%20image%2020250312140112.png)
### 1-1. 데이터 증강의 필요성

- **데이터 불균형 해소:**  
    처음 수집한 데이터는 양이 적고 편향된 경우가 많습니다. 증강을 통해 다양한 변형을 주어 데이터 분포를 고르게 하고, 모델의 일반화 성능을 향상시킬 수 있습니다.
    
- **다양성 증가:**  
    다양한 각도, 밝기, 배경 등 여러 조건에서의 변형 데이터를 생성하여, 실제 환경에서 발생할 수 있는 다양한 상황에 대응할 수 있습니다.
    

### 1-2. OCR 태스크에서의 Augmentation 시 주의 사항

일반적인 geometric transform (Random Crop, Random Rotation 등)을 OCR에 그대로 적용할 경우 문제가 발생할 수 있습니다.

- **문제점:**
    - **글자 잘림:** 증강 과정에서 텍스트 영역(개체)이 잘리거나 일부만 포함되는 경우, 모델 학습 시 혼란을 초래하거나 positive sample의 비율이 낮아져 성능이 저하될 수 있습니다.
    - **배경과의 hard negative sampling 부족:**  
        개체를 반드시 포함하도록 규칙을 적용하면, 텍스트와 멀리 있는 배경에서는 hard negative sampling이 어려워질 수 있습니다.
    - **밀집 영역 편향:**  
        개체 잘림을 방지하기 위한 규칙으로 인해, 밀집된 영역에서 샘플링이 어려워져 데이터 분포에 bias가 생길 수 있습니다.

### 1-3. 해결 방안

- **Augmentation 전후 규칙 적용:**
    - **최소 1개 이상 텍스트 개체 포함:** 증강 시, 이미지 내 최소한 하나의 텍스트 영역이 반드시 포함되도록 합니다.
    - **개체 잘림 방지:** 이미지 증강 후 잘린 텍스트 영역은 마스킹 처리하여 학습에서 무시할 수 있도록 설정합니다.
- **Hard Negative Mining 기법:**  
    배경 영역에서 어려운 샘플을 별도로 추출하여 학습에 활용하면, 전체 데이터셋의 다양성을 보완할 수 있습니다.

---

## 2. 합성 데이터 제작

### 2-1. 합성 데이터 제작의 필요성

- **데이터 양 증대:**  
    실제 데이터를 대량으로 수집하는 것은 비용과 시간이 많이 소요되므로, 합성 데이터를 통해 데이터를 증강할 수 있습니다.
    
- **개인정보 및 라이센스 제약 해소:**  
    합성 데이터는 실제 데이터와 달리 개인정보나 라이센스 문제에서 자유롭고, 원하는 형태의 세밀한 annotation을 얻을 수 있습니다.
    
- **비용 효율성:**  
    합성 데이터를 제작하는 비용은 상대적으로 낮으며, 다양한 환경과 조건을 쉽게 생성할 수 있습니다.
    

### 2-2. 합성 데이터 제작 도구

다음과 같은 도구들을 활용하여 OCR 합성 데이터를 제작할 수 있습니다.

- **TextRecognitionDataGenerator:**  
    텍스트 인식을 위한 합성 데이터 생성 도구.
- **SynthText:**  
    자연스러운 텍스트 배경 합성을 위한 대표적인 도구.
- **SynthText3D:**  
    3D 환경에서 텍스트를 합성하여 다양한 각도와 왜곡을 적용할 수 있는 도구.
- **UnrealText:**  
    Unreal Engine을 활용하여 보다 사실적인 합성 데이터 제작 가능.

### 2-3. 합성 데이터 활용 전략

1. **Pre-training:**  
    합성 데이터로 모델을 한 번 더 사전 학습(pre-train)하여, 기본적인 텍스트 인식 능력을 향상시킵니다.
2. **Fine-tuning:**  
    이후 실제 수집한 train dataset으로 모델을 fine-tuning하여, 합성 데이터와 실제 데이터 간의 차이를 보완합니다.

---

## 결론

OCR 태스크에서 모델 성능을 극대화하기 위해서는 좋은 데이터의 확보가 매우 중요합니다.

- **Augmentation:**  
    적절한 데이터 증강 기법을 통해, 기존의 제한적 데이터를 다양하고 균형 있게 확장할 수 있습니다. 다만, OCR 특성에 맞게 텍스트 개체가 잘리지 않도록 세심하게 설계해야 합니다.
- **합성 데이터 제작:**  
    합성 데이터를 활용하면, 비용 효율적이고 다양한 조건의 데이터를 대량으로 확보할 수 있어, pre-training과 fine-tuning을 통한 성능 향상에 큰 도움이 됩니다.

이와 같이, 데이터 중심 AI 접근법을 통해 보다 높은 품질의 데이터를 확보하면, OCR 모델의 성능을 지속적으로 개선하고 안정적인 서비스 운영이 가능해질 것입니다.

---