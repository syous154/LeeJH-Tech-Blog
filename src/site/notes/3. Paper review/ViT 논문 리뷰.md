---
{"dg-publish":true,"permalink":"/3-paper-review/vi-t/","tags":["Paper"],"created":"2025-02-26T15:44:19.167+09:00","updated":"2025-01-08T20:11:32.682+09:00"}
---

# 1. Abstract

- **문제 제기**: 컴퓨터 비전 분야에서 Transformer의 응용이 제한적이며, 주로 CNN과 함께 사용되거나 CNN의 일부 구성 요소를 대체하는 데 사용됩니다.
- **연구 목표**: CNN에 의존하지 않는 순수한 Transformer가 이미지 분류 작업에서 얼마나 성능을 낼 수 있는지 검증합니다.
- **방법**: 이미지 패치를 시퀀스로 처리하는 순수 Transformer 모델(Vision Transformer, ViT)을 사용하여 이미지 분류 작업 수행합니다.
- **결과**: ViT는 대규모 데이터에서 사전 학습한 후 다양한 크기의 이미지 인식 벤치마크에서 전이학습할 때, 최신 CNN 모델들과 비교하여 적은 계산 자원으로도 우수한 성능을 발휘합니다.
- **의의**: ViT는 컴퓨터 비전에서 CNN에 대한 의존성을 줄이고, Transformer 기반의 접근이 가능하다는 것을 입증했습니다.

# 2. Introduction

- **문제 배경**: Transformer 아키텍처는 NLP에서 널리 사용되며, 특히 대규모 사전 학습과 소규모 미세 조정을 통해 탁월한 성능을 보입니다. 반면 컴퓨터 비전에서는 CNN 아키텍처가 여전히 우세합니다.
- **연구 동기**: Transformer의 NLP 성공을 컴퓨터 비전에 적용해 보고자 하는 동기에서 출발했습니다. 특히, CNN 없이 Transformer를 이미지 인식에 적용할 수 있는 가능성을 탐구합니다.
- **연구 방법**: 표준 Transformer 모델을 최소한의 수정으로 이미지에 직접 적용합니다. 이미지를 작은 패치로 분할하고, 이를 Transformer 입력으로 제공하여 이미지 분류 작업을 수행합니다.
- **초기 결과**: 중간 크기의 데이터셋에서 Transformer는 CNN보다 낮은 성능을 보입니다. 이는 Transformer가 CNN의 전이 동등성(translation equivariance)과 지역성(locality)과 같은 귀납적 편향이 없기 때문입니다.
- **대규모 학습 결과**: 더 큰 데이터셋에서 Transformer를 훈련할 경우, 귀납적 편향의 부족을 극복하고, 최첨단 성능을 달성합니다. ViT는 대규모 데이터에서 사전 학습 후, 적은 양의 데이터로 전이학습할 때 매우 우수한 성능을 발휘합니다.
- **결론**: 충분한 데이터가 제공되면, 순수 Transformer 기반의 모델이 CNN 없이도 컴퓨터 비전 작업에서 우수한 성능을 낼 수 있음을 입증했습니다. ViT는 최신 CNN 모델과 비교하여 여러 이미지 인식 벤치마크에서 뛰어난 성과를 보여주었습니다.

# 3. METHOD

- 가능한 한 원래의 Transformer를 최대한 따라함으로써 NLP 아키텍처와 그 효율적인 구현을 거의 수정 없이 사용할 수 있게 됩니다.
![images/ViT images/image.png](/img/user/images/ViT%20images/image.png)

## 3.1 Vision Transformer(ViT)

- **Vision Transformer(ViT) 구조**:
    - 이미지를 작은 패치로 나누어, 이 패치를 Transformer 입력 시퀀스로 처리.
    - 패치 임베딩을 생성하여 Transformer에 입력으로 제공하며, BERT와 유사하게 학습 가능한 [class] 토큰을 앞에 추가하여 이미지 전체의 표현으로 사용.
- **구성 요소**:
    - **패치 임베딩**: 이미지를 작은 패치로 나누고 이를 평탄화하여 선형 투영을 통해 Transformer의 입력으로 변환합니다. Transformer는 기본적으로 1D 입력을 받기 때문에 패치 임베딩이 필요합니다.
        
        > 이미지$(H,W,C)$ → 패치$(N, P^2, C)$ → 평탄화된 2D 패치$(N, P^2⋅C)$
        > 
    - **위치 임베딩**: 패치 임베딩에 위치 정보를 추가하기 위해 표준 1D 위치 임베딩 사용합니다.
    2D 위치 임베딩을 사용하지 않는 이유는 좋은 성과를 보여주지 않았기 때문입니다.
    - **Transformer 인코더**: Multiheaded Self-Attention(MSA)과 MLP 블록으로 구성, 각 블록 전에 레이어 정규화(LN)와 잔차 연결 적용되고 GELU 비션형을 가진 두개의 층으로 구성됩니다.
        
        ![image.png](/img/user/images/ViT images/image 1.png)
        
- **귀납적 편향의 차이**:
    - ViT는 CNN에 비해 이미지 특화된 귀납적 편향이 적으며, 대부분의 공간적 관계를 처음부터 학습해야 합니다.
- **하이브리드 모델**:
    - ViT는 CNN의 특징 맵에서 생성된 패치를 입력 시퀀스로 사용하여 CNN과 Transformer를 결합한 하이브리드 아키텍처로도 사용 가능합니다.

## 3.2 미세 조정과 높은 해상도(Fine-Tuning and Higher Resolution)

- **미세 조정(fine-tuning) 방법**:
    - ViT는 대규모 데이터셋에서 사전 학습한 후, 더 작은 크기의 하류 작업(downstream tasks)으로 fine-tunning됩니다.
    - 사전 학습된 예측 헤드를 제거하고, 0으로 초기화된 새로운 피드포워드(feedforward) 계층을 추가하여 하류 클래스에 맞춤화합니다.
- **해상도 조정의 중요성**:
    - fine-tunning 시, 더 높은 해상도로 이미지를 입력하는 것이 성능 향상에 도움이 될 수 있습니다.
    - 패치 크기를 유지하면서 더 높은 해상도의 이미지를 사용하면, 결과적으로 더 긴 시퀀스 길이가 생깁니다.
- **위치 임베딩의 조정**:
    - ViT는 임의의 시퀀스 길이를 처리할 수 있지만, 사전 학습된 위치 임베딩은 해상도가 변경되면 무의미해질 수 있습니다.
    - 이를 해결하기 위해, 원본 이미지의 위치를 기준으로 사전 학습된 위치 임베딩에 대해 2D 보간을 수행합니다.
- **귀납적 편향의 제한적 사용**:
    - Vision Transformer에서 2D 구조에 대한 귀납적 편향은 해상도 조정과 패치 추출 과정에서만 수동으로 적용됩니다.
    - ViT는 CNN과 달리 이미지의 2D 구조에 대한 사전 지식이 거의 없이 학습되며, 대부분의 공간적 관계를 데이터로부터 학습해야 합니다.

# 4. Conclusion

- **연구 개요**:
    - Transformer를 이미지 인식에 직접 적용하여, 초기 패치 추출 단계를 제외하고는 이미지 특화된 귀납적 편향을 도입하지 않았습니다.
    - 이미지를 패치의 시퀀스로 해석하고, 표준 Transformer 인코더로 처리하는 간단하면서도 확장 가능한 전략을 사용했습니다.
- **결과 요약**:
    - 대규모 데이터셋에서 사전 학습을 통해 ViT는 많은 이미지 분류 데이터셋에서 SOTA 수준의 성능을 달성하거나 이보다 더 좋은 성능을 보였습니다.
    - 사전 학습 비용이 상대적으로 저렴합니다.
- **향후 과제와 도전 과제**:
    - ViT를 검출 및 분할과 같은 다른 컴퓨터 비전 작업에 적용하는 것.
    - 자기 지도 사전 학습 방법의 추가 탐구. 초기 실험은 성능 향상을 보여주지만, 지도 학습과의 격차가 큼.
    - ViT의 추가적인 확장과 스케일링이 성능 향상으로 이어질 가능성.
- **의의**:
    - ViT는 기존 CNN에 비해 더 적은 귀납적 편향으로도 이미지 인식에서 효과적으로 작동할 수 있음을 보여주었습니다